# plot(data, pch = 16)
####### Step 2 ------ Support Vector Regression
# SVR Model
if(require(e1071)){
model <- svm(Y ~ X, data)
predicted_Y <- predict(model, data)
points(data$X, predicted_Y, col = "red", pch = 4)
# compute error
error <- data$Y - predicted_Y
svr_prediction_rmse <- rmse(error) # 3.157061
# okay now let's improve the performance by tuning
tune_result <- tune(svm, Y ~ X, data = data, ranges = list(epsilon = seq(0, 1, 0.1), cost = 2^(2:9)))
print(tune_result)
}
svr_prediction_rmse
###### Step 1 ---- Linear Regression Model
# Load the data from csv file
data <- read.csv(file = "regression.csv",sep = ",", header = T)
plot(data, pch = 16)
# create a linear regression model
linear_regression_model <- lm(Y ~ X, data)
# add the fitted line
abline(linear_regression_model)
# display the predictions
predicted_Y <- predict(linear_regression_model, data)
# compute RMSE (root mean square error)
rmse <- function(error){
sqrt(mean(error^2))
}
error <- linear_regression_model$residuals # same as data$Y - predicted_Y
predictionRMSE <- rmse(error)
predictionRMSE
# plot(data, pch = 16)
####### Step 2 ------ Support Vector Regression
# SVR Model
if(require(e1071)){
model <- svm(Y ~ X, data)
predicted_Y <- predict(model, data)
points(data$X, predicted_Y, col = "red", pch = 4)
# compute error
error <- data$Y - predicted_Y
svr_prediction_rmse <- rmse(error) # 3.157061
# okay now let's improve the performance by tuning
tune_result <- tune(svm, Y ~ X, data = data, ranges = list(epsilon = seq(0, 1, 0.1), cost = 2^(2:9)))
print(tune_result)
}
svr_prediction_rmse
plot(data, pch = 16)
# create a linear regression model
linear_regression_model <- lm(Y ~ X, data)
# add the fitted line
abline(linear_regression_model)
# display the predictions
predicted_Y <- predict(linear_regression_model, data)
# compute RMSE (root mean square error)
rmse <- function(error){
sqrt(mean(error^2))
}
error <- linear_regression_model$residuals # same as data$Y - predicted_Y
predictionRMSE <- rmse(error)
predictionRMSE
if(require(e1071)){
model <- svm(Y ~ X, data)
predicted_Y <- predict(model, data)
points(data$X, predicted_Y, col = "red", pch = 4)
# compute error
error <- data$Y - predicted_Y
svr_prediction_rmse <- rmse(error) # 3.157061
# okay now let's improve the performance by tuning
tune_result <- tune(svm, Y ~ X, data = data, ranges = list(epsilon = seq(0, 1, 0.1), cost = 2^(2:9)))
print(tune_result)
}
svr_prediction_rmse
plot(tune_result)
}
# draw the second tuning graph
tune_result <- tune(svm, Y ~ X, data = data, ranges = list(epsilon = seq(0, 1, 0.2), cost = 2^(2:9)))
print(tune_result)
plot(tune_result)
# draw the second tuning graph
tune_result <- tune(svm, Y ~ X, data = data, ranges = list(epsilon = seq(0, 1, 0.2), cost = 2^(2:9)))
print(tune_result)
plot(tune_result)
plot(data, pch = 16)
plot(data, pch = 16)
tuned_model <- tune_result$best.model
tuned_model_y <- predict(tuned_model, data)
points(data$X, predicted_Y, col = "red", pch = 6)
lines(data$X, predicted_Y, col = "red", pch = 6)
predicted_tuned_model_Y <- predict(tuned_model, data)
points(data$X, predicted_tuned_model_Y, col = "blue", pch = 8)
lines(data$X, predicted_tuned_model_Y, col = "blue", pch = 4)
error <- data$Y - predicted_tuned_model_Y
error
tuned_RMSE <- rmse(error)
tuned_RMSE
###### Step 1 ---- Linear Regression Model
# Load the data from csv file
data <- read.csv(file = "regression.csv",sep = ",", header = T)
plot(data, pch = 16)
# create a linear regression model
linear_regression_model <- lm(Y ~ X, data)
# add the fitted line
abline(linear_regression_model)
# display the predictions
predicted_Y <- predict(linear_regression_model, data)
# compute RMSE (root mean square error)
rmse <- function(error){
sqrt(mean(error^2))
}
error <- linear_regression_model$residuals # same as data$Y - predicted_Y
predictionRMSE <- rmse(error)
predictionRMSE
# plot(data, pch = 16)
####### Step 2 ------ Support Vector Regression
# SVR Model
if(require(e1071)){
model <- svm(Y ~ X, data)
svr_predicted_Y <- predict(model, data)
points(data$X, svr_predicted_Y, col = "red", pch = 4)
# compute error
error <- data$Y - svr_predicted_Y
svr_prediction_rmse <- rmse(error) # 3.157061
# okay now let's improve the performance by tuning
tune_result <- tune(svm, Y ~ X, data = data, ranges = list(epsilon = seq(0, 1, 0.1), cost = 2^(2:9)))
print(tune_result)
plot(tune_result)
# draw the second tuning graph
tune_result <- tune(svm, Y ~ X, data = data, ranges = list(epsilon = seq(0, 1, 0.2), cost = 2^(2:9)))
print(tune_result)
plot(tune_result)
plot(data, pch = 16)
tuned_model <- tune_result$best.model
predicted_tuned_model_Y <- predict(tuned_model, data)
points(data$X, svr_predicted_Y, col = "red", pch = 2)
lines(data$X, svr_predicted_Y, col = "red", pch = 2)
points(data$X, predicted_tuned_model_Y, col = "blue", pch = 4)
lines(data$X, predicted_tuned_model_Y, col = "blue", pch = 4)
error <- data$Y - predicted_tuned_model_Y
tuned_RMSE <- rmse(error)
}
svr_prediction_rmse
abline(linear_regression_model)
###### Step 1 ---- Linear Regression Model
# Load the data from csv file
data <- read.csv(file = "regression.csv",sep = ",", header = T)
plot(data, pch = 16)
# create a linear regression model
linear_regression_model <- lm(Y ~ X, data)
# add the fitted line
abline(linear_regression_model, col = "green")
# display the predictions
predicted_Y <- predict(linear_regression_model, data)
# compute RMSE (root mean square error)
rmse <- function(error){
sqrt(mean(error^2))
}
error <- linear_regression_model$residuals # same as data$Y - predicted_Y
predictionRMSE <- rmse(error)
predictionRMSE
####### Step 2 ------ Support Vector Regression
# SVR Model
if(require(e1071)){
model <- svm(Y ~ X, data)
svr_predicted_Y <- predict(model, data)
points(data$X, svr_predicted_Y, col = "red", pch = 4)
# compute error
error <- data$Y - svr_predicted_Y
svr_prediction_rmse <- rmse(error) # 3.157061
# okay now let's improve the performance by tuning
tune_result <- tune(svm, Y ~ X, data = data, ranges = list(epsilon = seq(0, 1, 0.1), cost = 2^(2:9)))
print(tune_result)
plot(tune_result)
# draw the second tuning graph
tune_result <- tune(svm, Y ~ X, data = data, ranges = list(epsilon = seq(0, 1, 0.2), cost = 2^(2:9)))
print(tune_result)
plot(tune_result)
plot(data, pch = 16)
tuned_model <- tune_result$best.model
predicted_tuned_model_Y <- predict(tuned_model, data)
points(data$X, svr_predicted_Y, col = "red", pch = 2)
lines(data$X, svr_predicted_Y, col = "red", pch = 2)
points(data$X, predicted_tuned_model_Y, col = "blue", pch = 4)
lines(data$X, predicted_tuned_model_Y, col = "blue", pch = 4)
error <- data$Y - predicted_tuned_model_Y
tuned_RMSE <- rmse(error)
}
svr_prediction_rmse
###### Step 1 ---- Linear Regression Model
# Load the data from csv file
data <- read.csv(file = "regression.csv",sep = ",", header = T)
plot(data, pch = 16)
# create a linear regression model
linear_regression_model <- lm(Y ~ X, data)
# add the fitted line
abline(linear_regression_model, col = "green")
# display the predictions
predicted_Y <- predict(linear_regression_model, data)
# compute RMSE (root mean square error)
rmse <- function(error){
sqrt(mean(error^2))
}
error <- linear_regression_model$residuals # same as data$Y - predicted_Y
predictionRMSE <- rmse(error)
predictionRMSE
####### Step 2 ------ Support Vector Regression
# SVR Model
if(require(e1071)){
model <- svm(Y ~ X, data)
svr_predicted_Y <- predict(model, data)
points(data$X, svr_predicted_Y, col = "red", pch = 4)
# compute error
error <- data$Y - svr_predicted_Y
svr_prediction_rmse <- rmse(error) # 3.157061
# okay now let's improve the performance by tuning
tune_result <- tune(svm, Y ~ X, data = data, ranges = list(epsilon = seq(0, 1, 0.1), cost = 2^(2:9)))
print(tune_result)
plot(tune_result)
# draw the second tuning graph
tune_result <- tune(svm, Y ~ X, data = data, ranges = list(epsilon = seq(0, 1, 0.2), cost = 2^(2:9)))
print(tune_result)
plot(tune_result)
plot(data, pch = 16)
tuned_model <- tune_result$best.model
predicted_tuned_model_Y <- predict(tuned_model, data)
points(data$X, svr_predicted_Y, col = "red", pch = 2)
lines(data$X, svr_predicted_Y, col = "red", pch = 2)
points(data$X, predicted_tuned_model_Y, col = "blue", pch = 4)
lines(data$X, predicted_tuned_model_Y, col = "blue", pch = 4)
error <- data$Y - predicted_tuned_model_Y
tuned_RMSE <- rmse(error)
}
svr_prediction_rmse
abline(linear_regression_model, col = "green")
###### Step 1 ---- Linear Regression Model
# Load the data from csv file
data <- read.csv(file = "regression.csv",sep = ",", header = T)
plot(data, pch = 16)
# create a linear regression model
linear_regression_model <- lm(Y ~ X, data)
# add the fitted line
abline(linear_regression_model, col = "green")
# display the predictions
predicted_Y <- predict(linear_regression_model, data)
# compute RMSE (root mean square error)
rmse <- function(error){
sqrt(mean(error^2))
}
error <- linear_regression_model$residuals # same as data$Y - predicted_Y
predictionRMSE <- rmse(error)
predictionRMSE
####### Step 2 ------ Support Vector Regression
# SVR Model
if(require(e1071)){
model <- svm(Y ~ X, data)
svr_predicted_Y <- predict(model, data)
points(data$X, svr_predicted_Y, col = "red", pch = 4)
# compute error
error <- data$Y - svr_predicted_Y
svr_prediction_rmse <- rmse(error) # 3.157061
# okay now let's improve the performance by tuning
tune_result <- tune(svm, Y ~ X, data = data, ranges = list(epsilon = seq(0, 1, 0.1), cost = 2^(2:9)))
print(tune_result)
plot(tune_result)
# draw the second tuning graph
tune_result <- tune(svm, Y ~ X, data = data, ranges = list(epsilon = seq(0, 1, 0.2), cost = 2^(2:9)))
print(tune_result)
plot(tune_result)
plot(data, pch = 16)
tuned_model <- tune_result$best.model
predicted_tuned_model_Y <- predict(tuned_model, data)
points(data$X, svr_predicted_Y, col = "red", pch = 2)
lines(data$X, svr_predicted_Y, col = "red", pch = 2)
points(data$X, predicted_tuned_model_Y, col = "blue", pch = 4)
lines(data$X, predicted_tuned_model_Y, col = "blue", pch = 4)
error <- data$Y - predicted_tuned_model_Y
tuned_RMSE <- rmse(error)
}
svr_prediction_rmse
abline(linear_regression_model, col = "green")
tunedSVMModel <- tuned_model$best.model
tunedSVMModel <- tuned_model$best.model
print(tunedSVMModel)
tune_result$best.model
tuned_model
# Extract the coefficients (weights)
weights <- tuned_model $coefs[[1]]
w <- weights[2]  # Coefficient for the input feature 'X'
# Extract the intercept (bias)
b <- tuned_model $coefs[[1]][1]  # Intercept term
weights
w
weights[2]
weights
b
$coefs[[1]][1
coefs[[1]][1]
tuned_model $coefs[[1]][1]
# Extract the coefficients (weights)
weights <- tuned_model$coefs[[1]]
w <- weights[2]  # Coefficient for the input feature 'X'
# Extract the intercept (bias)
b <- tuned_model$coefs[[1]][1]  # Intercept term
w
# Extract the intercept (bias)
b <- tuned_model$coefs[[1]][1]  # Intercept term
b
View(data)
View(data)
###### Step 1 ---- Linear Regression Model
# Load the data from csv file
data <- read.csv(file = "regression.csv",sep = ",", header = T)
plot(data, pch = 16)
# create a linear regression model
linear_regression_model <- lm(Y ~ X, data)
# add the fitted line
abline(linear_regression_model, col = "green")
# display the predictions
predicted_Y <- predict(linear_regression_model, data)
# compute RMSE (root mean square error)
rmse <- function(error){
sqrt(mean(error^2))
}
error <- linear_regression_model$residuals # same as data$Y - predicted_Y
predictionRMSE <- rmse(error)
predictionRMSE
if(require(e1071)){
model <- svm(Y ~ X, data)
svr_predicted_Y <- predict(model, data)
points(data$X, svr_predicted_Y, col = "red", pch = 4)
# compute error
error <- data$Y - svr_predicted_Y
svr_prediction_rmse <- rmse(error) # 3.157061
# okay now let's improve the performance by tuning
tune_result <- tune(svm, Y ~ X, data = data, ranges = list(epsilon = seq(0, 1, 0.1), cost = 2^(2:9)))
print(tune_result)
plot(tune_result)
# draw the second tuning graph
tune_result <- tune(svm, Y ~ X, data = data, ranges = list(epsilon = seq(0, 1, 0.2), cost = 2^(2:9)))
print(tune_result)
plot(tune_result)
plot(data, pch = 16)
tuned_model <- tune_result$best.model
predicted_tuned_model_Y <- predict(tuned_model, data)
points(data$X, svr_predicted_Y, col = "red", pch = 2)
lines(data$X, svr_predicted_Y, col = "red", pch = 2)
points(data$X, predicted_tuned_model_Y, col = "blue", pch = 4)
lines(data$X, predicted_tuned_model_Y, col = "blue", pch = 4)
error <- data$Y - predicted_tuned_model_Y
tuned_RMSE <- rmse(error)
# now trying to find the functions coefficients
# y = w * x + b
# Extract the coefficients (weights)
weights <- tuned_model$coefs[[1]]
w <- weights[2]  # Coefficient for the input feature 'X'
# Extract the intercept (bias)
b <- tuned_model$coefs[[1]][1]  # Intercept term
}
svr_prediction_rmse
svr_prediction_rmse
svr_prediction_rmse
# linear regression line
abline(linear_regression_model, col = "green")
b
w
# Extract the coefficients (weights)
weights <- tuned_model$coefs[[1]]
weights
# Extract the coefficients (weights)
weights <- tuned_model$coefs
w <- weights[2]  # Coefficient for the input feature 'X'
# Extract the intercept (bias)
b <- tuned_model$rho  # Intercept term
w
b
tuned_model$coefs
w
View(data)
###### Step 1 ---- Linear Regression Model
# Load the data from csv file
data <- read.csv(file = "regression.csv",sep = ",", header = T)
###### Step 1 ---- Linear Regression Model
# Load the data from csv file
data <- read.csv(file = "regression.csv",sep = ",", header = T)
plot(data, pch = 16)
# create a linear regression model
linear_regression_model <- lm(Y ~ X, data)
# add the fitted line
abline(linear_regression_model, col = "green")
# display the predictions
predicted_Y <- predict(linear_regression_model, data)
# compute RMSE (root mean square error)
rmse <- function(error){
sqrt(mean(error^2))
}
error <- linear_regression_model$residuals # same as data$Y - predicted_Y
predictionRMSE <- rmse(error)
predictionRMSE
if(require(e1071)){
model <- svm(Y ~ X, data)
svr_predicted_Y <- predict(model, data)
points(data$X, svr_predicted_Y, col = "red", pch = 4)
# compute error
error <- data$Y - svr_predicted_Y
svr_prediction_rmse <- rmse(error) # 3.157061
# okay now let's improve the performance by tuning
tune_result <- tune(svm, Y ~ X, data = data, ranges = list(epsilon = seq(0, 1, 0.1), cost = 2^(2:9)))
print(tune_result)
# plot(tune_result)
# draw the second tuning graph
tune_result <- tune(svm, Y ~ X, data = data, ranges = list(epsilon = seq(0, 1, 0.2), cost = 2^(2:9)))
print(tune_result)
# plot(tune_result)
plot(data, pch = 16)
tuned_model <- tune_result$best.model
predicted_tuned_model_Y <- predict(tuned_model, data)
points(data$X, svr_predicted_Y, col = "red", pch = 2)
lines(data$X, svr_predicted_Y, col = "red", pch = 2)
points(data$X, predicted_tuned_model_Y, col = "blue", pch = 4)
lines(data$X, predicted_tuned_model_Y, col = "blue", pch = 4)
error <- data$Y - predicted_tuned_model_Y
tuned_RMSE <- rmse(error)
# now trying to find the functions coefficients
# y = w * x + b
# Extract the coefficients (weights)
weights <- tuned_model$coefs
w <- weights[2]  # Coefficient for the input feature 'X'
# Extract the intercept (bias)
b <- tuned_model$rho  # Intercept term
}
tuned_model$coefs
w
tuned_model$rho
svr_prediction_rmse
# linear regression line
abline(linear_regression_model, col = "green")
library(ISLR)
library(ISLR)
install.packages("ISLR")
install.packages("ISLR")
library(ISLR)
library(dplyr)
library(ggplot2)
library(earth)
install.packages("earth")
library(earth)
library(caret)
head(Wage)
hyper_grid <- expand.grid(degree = 1:3, nprune = seq(2, 50, length.out = 10) %>% floor())
set.seed(1)
cv_mars <- train(
x = subset(Wage, select = -c(wage, logwage)),
y = Wage$wage,
method = "earth",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid
)
cv_mars <- train(
x = subset(Wage, select = -c(wage, logwage)),
y = Wage$wage,
method = "earth",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid
)
# display model with lowest test RMSE
cv_mars$results %>%
filter(nprune == cv_mars$bestTune$nprune, degree = cv_mars$bestTune$degree)
# display model with lowest test RMSE
cv_mars$results %>%
filter(nprune == cv_mars$bestTune$nprune, degree = cv_mars$bestTune$degree)
install.packages("ISLR")
install.packages("earth")
library(ISLR)
library(dplyr)
library(ggplot2)
library(earth)
library(caret)
head(Wage) # view the data
# Build and Optimize the MARS Model
hyper_grid <- expand.grid(degree = 1:3, nprune = seq(2, 50, length.out = 10) %>% floor())
set.seed(1)
# fit MARS model using k-fold cross-validation
cv_mars <- train(
x = subset(Wage, select = -c(wage, logwage)),
y = Wage$wage,
method = "earth",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid
)
head(Wage) # view the data
# Build and Optimize the MARS Model
hyper_grid <- expand.grid(degree = 1:3, nprune = seq(2, 50, length.out = 10) %>% floor())
set.seed(1)
# fit MARS model using k-fold cross-validation
cv_mars <- train(
x = subset(Wage, select = -c(wage, logwage)),
y = Wage$wage,
method = "earth",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = hyper_grid
)
# display model with lowest test RMSE
cv_mars$results %>%
filter(nprune == cv_mars$bestTune$nprune, degree = cv_mars$bestTune$degree)
# display model with lowest test RMSE
cv_mars$results %>%
filter(nprune == cv_mars$bestTune$nprune, degree == cv_mars$bestTune$degree)
ggplot(cv_mars)
